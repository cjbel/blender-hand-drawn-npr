% ~1 page total.


@inproceedings{hertzmann2002
author={Hertzmann,A.},
title={Stroke-based Rendering},
booktitle={Advances in NPR for Art and Visualization},
year={2002}
}


% A more consise version of hertzmann2002.
@misc{hertzmann2003
author={Hertzmann,A.},
year={2003},
title={A survey of stroke-based rendering},
journal={IEEE Computer Graphics and Applications},
volume={23},
number={4},
pages={70-81},
abstract={This tutorial describes several stroke-based rendering (SBR) algorithms. SBR is an automatic approach to creating nonphotorealistic imagery by placing discrete elements such as paint strokes or stipples.},
keywords={Greedy algorithms; Algorithm design and analysis; Art; Humans; Automatic control; Rendering (computer graphics); Data structures; Control systems; Iterative algorithms; Paints; Image processing; Computer graphics; Research},
isbn={0272-1716},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{girshick2000
author={Girshick,Ahna and Interrante,Victoria and Haker,Steven and Lemoine,Todd},
editor={ },
year={2000},
title={Line direction matters: an argument for the use of principal directions in 3D line drawings},
publisher={ACM},
pages={43-52},
keywords={line drawings; non-photorealistic rendering; geometrically invariant line drawings; principal direction line drawings},
isbn={1581132778;9781581132779;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{hertzmann2000
author={Hertzmann,Aaron and Zorin,Denis},
editor={ },
year={2000},
title={Illustrating smooth surfaces},
publisher={ACM Press/Addison-Wesley Publishing Co},
pages={517-526},
abstract={We present a new set of algorithms for line-art rendering of smooth surfaces. We introduce an efficient, deterministic algorithm for finding silhouettes based on geometric duality, and an algorithm for segmenting the silhouette curves into smooth parts with constant visibility. These methods can be used to find all silhouettes in real time in software. We present an automatic method for generating hatch marks in order to convey surface shape. We demonstrate these algorithms with a drawing style inspired by A Topological Picturebook by G. Francis.},
keywords={pen-and-ink illustration; non-photorealistic rendering; direction fields; hatching; silhouettes},
isbn={9781581132083;1581132085;},
language={English},
}


% Cited from hertzmann2002.
@article{haeberli1990
author={Haeberli,Paul},
year={1990},
title={Paint by numbers: abstract image representations},
journal={ACM SIGGRAPH Computer Graphics},
volume={24},
number={4},
pages={207-214},
isbn={0097-8930},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{salisbury1994
author={Salisbury,Michael and Anderson,Sean and Barzel,Ronen and Salesin,David},
editor={ },
year={1994},
title={Interactive pen-and-ink illustration},
publisher={ACM},
pages={101-108},
abstract={We present an interactive system for creating pen-and-ink illustrations. The system uses stroke textures -collections of strokes arranged in different patterns-to generate texture and tone. The user "paints" with a desired stroke texture to achieve a desired tone, and the computer draws all of the individual strokes. The system includes support for using scanned or rendered images for reference to provide the user with guides for outline and tone. By following these guides closely, the illustration system can be used for interactive digital halftoning, in which stroke textures are applied to convey details that would otherwise be lost in this black-and-white medium. By removing the burden of placing individual strokes from the user, the illustration system makes it possible to create fine stroke work with a purely mouse-based interface. Thus, this approach holds promise for bringing high-quality black-and-white illustration to the world of personal computing and desktop publishing.},
keywords={non-photorealistic rendering; prioritized stroke textures; comprehensible rendering},
isbn={0897916670;9780897916677;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{salisbury1996
author={Salisbury,Mike and Anderson,Corin and Lischinski,Dani and Salesin,David},
editor={ },
year={1996},
title={Scale-dependent reproduction of pen-and-ink illustrations},
publisher={ACM},
pages={461-468},
keywords={image magnification; non-photorealistic rendering; stroke textures; discontinuity edges; image resampling; scale-dependent rendering},
isbn={9780897917469;0897917464;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{salisbury1997
author={Salisbury,Michael and Wong,Michael and Hughes,John and Salesin,David},
editor={ },
year={1997},
title={Orientable textures for image-based pen-and-ink illustration},
publisher={ACM Press/Addison-Wesley Publishing Co},
pages={401-406},
keywords={non-photorealistic rendering; stroke textures; direction field; image-based rendering; controlled-density hatching; scale-dependent rendering},
isbn={0897918967;9780897918961;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{winkenbach1994
author={Winkenbach,Georges and Salesin,David},
editor={ },
year={1994},
title={Computer-generated pen-and-ink illustration},
publisher={ACM},
pages={91-100},
abstract={This paper describes the principles of traditional pen-and-ink illustration, and shows how a great number of them can be implemented as part of an automated rendering system. It introduces "stroke textures," which can be used for achieving both texture and tone with line drawing. Stroke textures also allow resolution-dependent rendering, in which the choice of strokes used in an illustration is appropriately tied to the resolution of the target medium. We demonstrate these techniques using complex architectural models, including Frank Lloyd Wright's "Robie House."},
keywords={non-photorealistic rendering; prioritized stroke textures; architectural rendering; comprehensible rendering; resolution-dependent rendering; texture indication},
isbn={0897916670;9780897916677;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{winkenbach1996
author={Winkenbach,Georges and Salesin,David},
editor={ },
year={1996},
title={Rendering parametric surfaces in pen and ink},
publisher={ACM},
pages={469-476},
keywords={outlining; pen-and-ink rendering; shadow algorithms; resolution-dependent rendering; controlled-density hatching; non-photorealistic rendering; stroke textures; comprehensible rendering},
isbn={9780897917469;0897917464;},
language={English},
}


% Cited from hertzmann2002, find this:
% Bruno Jobard and Wilfrid Lefer. Creating evenly-spaced streamlines of arbitrary density.
% In Proc. of 8th Eurographics Workshop on Visualization in Scientific Computing, pages
% 45–55, 1997.


% Cited from hertzmann2002.
@inproceedings{
author={Kowalski,Michael and Markosian,Lee and Northrup,J. and Bourdev,Lubomir and Barzel,Ronen and Holden,Loring and Hughes,John},
editor={ },
year={1999},
title={Art-based rendering of fur, grass, and trees},
publisher={ACM Press/Addison-Wesley Publishing Co},
pages={433-438},
keywords={non-photorealistic rendering; graftals; procedural textures},
isbn={9780201485608;0201485605;},
language={English},
}


% Cited from hertzmann2002.
@inproceedings{
author={Gooch,Bruce and Coombe,Greg and Shirley,Peter},
editor={ },
year={2002},
title={Artistic Vision: painterly rendering using computer vision techniques},
publisher={ACM},
pages={83-ff},
abstract={We present a method that takes a raster image as input and produces a painting-like image composed of strokes rather than pixels. Our method works by first segmenting the image into features, finding the approximate medial axes of these features, and using the medial axes to guide brush stroke creation. System parameters may be interactively manipulated by a user to effect image segmentation, brush stroke characteristics, stroke size, and stroke frequency. This process creates images reminiscent of those contemporary representational painters whose work has an abstract or sketchy quality. Our software is available at http://www.cs.utah.edu/npr/ArtisticVision.},
keywords={image processing; medial axis; non-photorealistic rendering; image moments; painting},
isbn={1581134940;9781581134940;},
language={English},
}


% =============================================================================


@article{saito1990
author={Saito,Takafumi and Takahashi,Tokiichiro},
year={1990},
title={Comprehensible rendering of 3-D shapes},
journal={ACM SIGGRAPH Computer Graphics},
volume={24},
number={4},
pages={197-206},
isbn={0097-8930},
language={English},
}


% Provides a good overview of related work.
@article{
author={Kwon,Yunmi and Yang,Heekyung and Min,Kyungha},
year={2012},
title={Pencil rendering on 3D meshes using convolution},
journal={Computers & Graphics},
volume={36},
number={8},
pages={930-944},
abstract={We produce various styles of pencil drawings from a 3D triangular mesh using a new two-phase approach based on convolution. First, we generate the noise particles and integration directions required for convolution on the mesh and project them onto the image space. We then use the improved convolution algorithm to integrate the projected noise along the generated integration directions. This scheme produces pencil drawings in different styles, including feature-conveying, monochrome tone-depicting and smooth color-depicting styles, examples of which are provided in this study. This rendering process is temporally coherent. Therefore, it can be used to create drawing-styled animations.},
keywords={Non-photorealistic rendering; Convolution; Pencil drawing; Temporal coherence; Triangular mesh; Usage; Algorithms},
isbn={0097-8493},
language={English},
}


% Cited from Kwon, Pencil Rendering From 3D models:

% ***
@article{
author={Zander,Johannes and Isenberg,Tobias and Schlechtweg,Stefan and Strothotte,Thomas},
year={2004},
title={High Quality Hatching},
journal={Computer Graphics Forum},
volume={23},
number={3},
pages={421-430},
abstract={Hatching lines are often used in line illustrations to convey tone and texture of a surface. In this paper we present methods to generate hatching lines from polygonal meshes and render them in high quality either at interactive rates for on-screen display or for reproduction in print. Our approach is based on local curvature information that is integrated to form streamlines on the surface of the mesh. We use a new algorithm that provides an even distribution of these lines. A special processing of these streamlines ensures high quality line rendering for both intended output media later on. While the streamlines are generated in a preprocessing stage, hatching lines are rendered either for vector-based printer output or on-screen display, the latter allowing for interaction in terms of changing the view parameters or manipulating the entire line shading model at run-time using a virtual machine. [PUBLICATION ABSTRACT]},
keywords={Studies; Algorithms; Computer graphics},
isbn={0167-7055},
language={English},
}


% ***
@inproceedings{
author={Lee,Hyunjun and Kwon,Sungtae and Lee,Seungyong},
editor={ },
year={2006},
title={Real-time pencil rendering},
publisher={ACM},
pages={37-45},
abstract={This paper presents a real-time technique for rendering 3D meshes in the pencil drawing style. We analyze the characteristics of pencil drawing and incorporate them into the rendering process, which is fully implemented on a GPU. For object contours, we propose a multiple contour drawing technique that imitates trial-and-errors of human in contour drawing. For interior shading, we present a simple approach for mapping oriented textures onto an object surface. To improve the quality of pencil rendering, we generate and map pencil textures that reflect the properties of graphite pencils and paper. We show several rendering examples that demonstrate the high performance of the proposed technique in terms of speed and quality.},
keywords={pencil drawing; non-photorealistic rendering; pencil texture; multiple contour drawing; GPU rendering},
isbn={1595933573;9781595933577;},
language={English},
}


% ***
@article{
author={Kim,Yongjin and Yu,Jingyi and Yu,Xuan and Lee,Seungyong},
year={2008},
title={Line-art illustration of dynamic and specular surfaces},
journal={ACM Transactions on Graphics},
volume={27},
number={5},
pages={1},
isbn={0730-0301},
language={English},
}


% **
@article{
author={Paiva,Afonso and Vital Brazil,Emilio and Petronetto,Fabiano and Sousa,Mario C.},
year={2009},
title={Fluid-based hatching for tone mapping in line illustrations},
journal={The Visual Computer},
volume={25},
number={5},
pages={519-527},
abstract={This paper presents a novel meshless, physically-based framework for line art rendering of surfaces with complex geometry and arbitrary topology. We apply an inviscid fluid flow simulation using Smoothed Particles Hydrodynamics to compute the global velocity and cross fields over the surface model. These fields guide the automatic placement of strokes while extracting the geometric and topological coherence of the model. Target tones are matched by tonal value maps allowing different hatching and cross-hatching effects. We demonstrate the simplicity and effectiveness of our method with sample renderings obtained for a variety of models.},
keywords={Computer Graphics; Computational fluid dynamics; Computer Science; Image Processing and Computer Vision; Artificial Intelligence (incl. Robotics); Non-photorealistic rendering; Pen and ink hatching; Direction fields; Smoothed particles hydrodynamics; Fluid dynamics; Analysis},
isbn={0178-2789},
language={English},
}


% Cited from Kwon, Pencil rendering from images

@inproceedings{
author={Li,Nan and Huang,Zhiong},
editor={ },
year={2003},
title={A feature-based pencil drawing method},
publisher={ACM},
pages={135-ff},
abstract={We present a method for generating a pencil drawing from a digital image using the feature geometric attributes obtained by analysis of image moment and texture of each region. Thus, the drawing results represent not only the local region information but also the feature characteristics. It is inspired and devised to correspond to a real pencil drawing process: A painter often divides a scene into regions, observes the distinctive features, and applies the pencil strokes accordingly. We have implemented the method and compared the results with Adobe Photoshop 5.0.},
keywords={non-photo realistic rendering; enhanced realistic; image moment function; texture analysis},
isbn={9781581135787;1581135785;},
language={English},
}


@inproceedings{
author={Yamamoto,S. and Mo,Xiaoyang and Imamiya,A.},
editor={ },
year={2004},
title={Enhanced LIC pencil filter},
publisher={IEEE},
pages={251-256},
abstract={This paper proposes an extension to the existing automatic pencil drawing generation technique based on line integral convolution (LIC). The original LIC pencil filter utilizes image segmentation and texture direction detection techniques for defining outlines and stroke directions, and the quality of a resulting image depends largely on the result of image segmentation. It may fail to generate a reasonable result when the segmentation result is not consistent with the structure of the input image. To solve this problem, we propose in this paper to avoid the explicit region subdivision. Instead, we divide a source image into layers of successive intensity ranges, generate a stroke image for each layer, and add them together to obtain the final pencil drawing.},
keywords={Image segmentation; Filters; Filtering; Image converters; Convolution; Image generation; Ink; Rendering (computer graphics); Data systems; Painting},
isbn={9780769521787;0769521789;},
language={English},
}

% ---


@inproceedings{
author={Praun,Emil and Hoppe,Hugues and Webb,Matthew and Finkelstein,Adam},
editor={ },
year={2001},
title={Real-time hatching},
publisher={ACM},
pages={581},
abstract={Drawing surfaces using hatching strokes simultaneously conveys material, tone, and form. We present a real-time system for non-photorealistic rendering of hatching strokes over arbitrary surfaces. During an automatic preprocess, we construct a sequence of mipmapped hatch images corresponding to different tones, collectively called a tonal art map . Strokes within the hatch images are scaled to attain appropriate stroke size and density at all resolutions, and are organized to maintain coherence across scales and tones. At runtime, hardware multitexturing blends the hatch images over the rendered faces to locally vary tone while maintaining both spatial and temporal coherence. To render strokes over arbitrary surfaces, we build a lapped texture parametrization where the overlapping patches align to a curvature-based direction field. We demonstrate hatching strokes over complex surfaces in a variety of styles.},
keywords={non-photorealistic rendering; chicken-and-egg problem; line art; multitexturing},
isbn={9781581133745;158113374X;},
language={English},
}


@article{
author={Suarez,Jordane and Belhadj,Farès and Boyer,Vincent},
year={2017},
title={Real-time 3D rendering with hatching},
journal={The Visual Computer},
volume={33},
number={10},
pages={1319-1334},
abstract={We present an approach for real-time pen-and-ink hatching renderings on large scenes. Starting with 3D models including photorealistic textures and materials, we aim to propose a solution that produces hatched renderings. As we consider scene objects described as polygonal meshes with their own textures, we produce once hatching patterns at different tones and resolutions considering the material of each object. To achieve that, we create a flow direction map per texture pixel, using contour characteristics extracted from the original texture and then interpolated. Stroke trajectories are thus generated depending on the flow direction and using B-splines, providing tones from light-to-dark. Tones are then stored in a mutli-resolution tonal art map. Moreover, we aim to overcome the limitations of existing hatching rendering methods by introducing an illumination model, fully implemented on GPU and able to manage three shading types: regular shadow, soft/cast shadow and self-shadowing. Tones and hatching resolutions are, therefore, assigned according to local/global illumination supporting multiple light sources. Our model, both dedicated for 3D static model renderings and 3D model animation, supports model deformations and is also spatially and temporally coherent since it gives continuous hatching strokes during object animations and/or light displacements.},
keywords={Stylized rendering and animation; Computer Graphics; Real-time rendering; Hatching; Computer Science; Image Processing and Computer Vision; Artificial Intelligence (incl. Robotics); Computer Science, general; GPU; Deformation; Splines; Shading; Strokes; Photorealistic; Real time; Texture; Light sources; Rendering; Animation; Illumination; Static models; Three dimensional models},
isbn={0178-2789},
language={English},
}


@article{
author={Gerl,Moritz and Isenberg,Tobias},
year={2013},
title={Interactive example-based hatching},
journal={Computers & Graphics},
volume={37},
number={1-2},
pages={65-80},
abstract={We present an approach for interactively generating pen-and-ink hatching renderings based on hand-drawn examples. We aim to overcome the regular and synthetic appearance of the results of existing methods by incorporating human virtuosity and illustration skills in the computer generation of such imagery. To achieve this goal, we propose to integrate an automatic style transfer with user interactions. This approach leverages the potential of example-based hatching while giving users the control and creative freedom to enhance the aesthetic appearance of the results. Using a scanned-in hatching illustration as input, we use image processing and machine learning methods to learn a model of the drawing style in the example illustration. We then apply this model to semi-automatically synthesize hatching illustrations of 3D meshes in the learned drawing style. In the learning stage, we first establish an analytical description of the hand-drawn example illustration using image processing. A 3D scene registered with the example drawing allows us to infer object-space information related to the 2D drawing elements. We employ a hierarchical style transfer model that captures drawing characteristics on four levels of abstraction, which are global, patch, stroke, and pixel levels. In the synthesis stage, an explicit representation of hatching strokes and hatching patches enables us to synthesize the learned hierarchical drawing characteristics. Our representation makes it possible to directly and intuitively interact with the hatching illustration. Amongst other interactions, users of our system can brush with patches of hatching strokes onto a 3D mesh. This interaction capability allows illustrators who are working with our system to make use of their artistic skills. Furthermore, the proposed interactions allow people without a background in hatching to interactively generate visually appealing hatching illustrations.;We present an approach for interactively generating pen-and-ink hatching renderings based on hand-drawn examples. We aim to overcome the regular and synthetic appearance of the results of existing methods by incorporating human virtuosity and illustration skills in the computer generation of such imagery. To achieve this goal, we propose to integrate an automatic style transfer with user interactions. This approach leverages the potential of example-based hatching while giving users the control and creative freedom to enhance the aesthetic appearance of the results. Using a scanned-in hatching illustration as input, we use image processing and machine learning methods to learn a model of the drawing style in the example illustration. We then apply this model to semi-automatically synthesize hatching illustrations of 3D meshes in the learned drawing style. In the learning stage, we first establish an analytical description of the hand-drawn example illustration using image processing. A 3D scene registered with the example drawing allows us to infer object-space information related to the 2D drawing elements. We employ a hierarchical style transfer model that captures drawing characteristics on four levels of abstraction, which are global, patch, stroke, and pixel levels. In the synthesis stage, an explicit representation of hatching strokes and hatching patches enables us to synthesize the learned hierarchical drawing characteristics. Our representation makes it possible to directly and intuitively interact with the hatching illustration. Amongst other interactions, users of our system can brush with patches of hatching strokes onto a 3D mesh. This interaction capability allows illustrators who are working with our system to make use of their artistic skills. Furthermore, the proposed interactions allow people without a background in hatching to interactively generate visually appealing hatching illustrations. (c) 2012 Elsevier Ltd. All rights reserved.;},
keywords={Style transfer; Illustrations by example; Interactive example-based; Learning hatching; Hatching by example; Example-based; Hatching; Non-photorealistic rendering; Illustrative rendering; Pen-and-ink; Interactive illustrative rendering; Image processing; Analysis; Equipment and supplies; Graphics; Computer Science},
isbn={0097-8493},
language={English},
}


@inproceedings{
author={Coconu,Liviu and Deussen,Oliver and Hege,Hans-Christian},
editor={ },
year={2006},
title={Real-time pen-and-ink illustration of landscapes},
publisher={ACM},
pages={27-35},
abstract={Non-photorealistic rendering has been proven to be particularly efficient in conveying and transmitting selected visual information. Our paper presents a NPR rendering pipeline that supports pen-and-ink illustration for, but not limited to, complex landscape scenes in real time. This encompasses a simplification framework using clustering which enables new approaches to efficient and coherent rendering of stylized silhouettes, hatching and abstract shading. Silhouette stylization is performed in image-space. This avoids explicit computation of connected lines. Further, coherent hatching of the tree foliage is performed using an approximate view-dependent parameterization computed on-the-fly within the same simplification framework. All NPR algorithms are integrated with photorealistic rendering, allowing seamless transition and combination between a variety of photorealistic and non-photorealistic drawing styles.},
keywords={complex plant scenes; level-of-detail; non-photorealistic rendering; outdoor scenes; real-time hatching; line art},
isbn={1595933573;9781595933577;},
language={English},
}


@article{
author={DeCarlo,Doug and Finkelstein,Adam and Rusinkiewicz,Szymon and Santella,Anthony},
year={2003},
title={Suggestive contours for conveying shape},
journal={ACM Transactions on Graphics},
volume={22},
number={3},
pages={848},
isbn={0730-0301},
language={English},
}


@article{
author={Nalbach,O. and Arabadzhiyska,E. and Mehta,D. and Seidel,H. ‐. and Ritschel,T.},
year={2017},
title={Deep Shading: Convolutional Neural Networks for Screen Space Shading},
journal={Computer Graphics Forum},
volume={36},
number={4},
pages={65-78},
abstract={In computer vision, convolutional neural networks (CNNs) achieve unprecedented performance for inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screen space shading has boosted the quality of real-time rendering, converting the same kind of attributes of a virtual scene back to appearance, enabling effects like ambient occlusion, indirect light, scattering and many more. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading renders screen space effects at competitive quality and speed while not being programmed by human experts but learned from example images.},
keywords={CCS Concepts; Computing methodologies → Neural networks; Rasterization; Rendering; Occlusion; Computer vision; Hardware reviews; Scattering; Shading; Real time; Pixels; Inverse problems; Converting; Neural networks; Virtual reality; Computer graphics; Reflectance},
isbn={0167-7055},
language={English},
}


@book{
author={Francis,George K.},
year={2007},
title={A topological picturebook},
publisher={Springer},
address={New York},
edition={1st soft-cover},
keywords={Graphic methods; Topology},
isbn={9780387681207;0387681205;},
language={English},
}


@article{
author={Elber,G.},
year={1998},
title={Line art illustrations of parametric and implicit forms},
journal={IEEE Transactions on Visualization and Computer Graphics},
volume={4},
number={1},
pages={71-81},
abstract={A technique is presented for line art rendering of scenes composed of freeform surfaces. The line art that is created for parametric surfaces is practically intrinsic and is globally invariant to changes in the surface parameterization. This method is equally applicable for line art rendering of implicit forms, creating a unified line art rendering method for both parametric and implicit forms. This added flexibility exposes a new horizon of special, parameterization independent, line art effects. Moreover, the production of the line art illustrations can be combined with traditional rendering techniques such as transparency and texture mapping. Examples that demonstrate the capabilities of the proposed approach are presented for both the parametric and implicit forms.},
keywords={Surface reconstruction; Art; Subspace constraints; Layout; Computer graphics; Production; Ray tracing; Rendering (computer graphics); Surface topography; Spline},
isbn={1077-2626},
language={English},
}


@inproceedings{
author={Lake,Adam and Marshall,Carl and Harris,Mark and Blackstein,Marc},
editor={ },
year={2000},
title={Stylized rendering techniques for scalable real-time 3D animation},
publisher={ACM},
pages={13-20},
keywords={cartoon rendering; pencil sketch rendering; cartoon effects; silhouette edge detection; real-time nonphotorealistic animation and rendering; stylized rendering},
isbn={1581132778;9781581132779;},
language={English},
}


@inproceedings{
author={Webb,Matthew and Praun,Emil and Finkelstein,Adam and Hoppe,Hugues},
editor={ },
year={2002},
title={Fine tone control in hardware hatching},
publisher={ACM},
pages={53-ff},
abstract={Recent advances in NPR have enabled real-time rendering of 3D models shaded with hatching strokes for use in interactive applications. The key challenges in real-time hatching are to convey tone by dynamically adjusting stroke density, while controlling stroke size and maintaining frame-to-frame coherence. In this paper, we introduce two new real-time hatching schemes that leverage recent advances in texture mapping hardware. Both schemes provide enhanced control of tone, thereby avoiding blending or aliasing artifacts present in previous systems. The first scheme, which relies on volume rendering hardware, admits the use of color. The second scheme, which uses pixel shaders, allows per-pixel lighting operations such as texture modulation. Both schemes run at interactive rates on inexpensive PC graphics cards.},
keywords={non-photorealistic rendering; line art; multitexturing},
isbn={1581134940;9781581134940;},
language={English},
}

